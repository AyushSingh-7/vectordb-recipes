{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain openai tiktoken lancedb\n",
        "!pip install  CTransformers\n",
        "!pip install sentence_transformers\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "dEXwR1YB1XqN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import openai\n",
        "import gradio as gr\n",
        "from typing import List, Union\n",
        "import lancedb\n",
        "import langchain\n",
        "from langchain.vectorstores import LanceDB\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader"
      ],
      "metadata": {
        "id": "zLglSbFW1XnY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3N56BUCyDuce",
        "outputId": "589d5dc2-6341-4619-fede-2fa5f7d9edb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download the model"
      ],
      "metadata": {
        "id": "NGtP2Nl613DN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwDvXiLv17fe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the mistral model & out it in your drive\n",
        "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
      ],
      "metadata": {
        "id": "2pAqRQOy1tH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### mount to path where weight is stored"
      ],
      "metadata": {
        "id": "JbIP6RZU2L1i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/llm/mistral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAVZKtay18Fz",
        "outputId": "b9861176-fd84-4953-dcc3-104643d68146"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llm/mistral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first time it will download the bge embedding model from HF"
      ],
      "metadata": {
        "id": "rHAz9usp240R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "sKfAF1Kk1LvO",
        "outputId": "130ddafe-107a-43d3-84cd-123b9b00db88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-5796db601f02>:119: GradioUnusedKwargWarning: You have unused kwarg parameters in Textbox, please remove them: {'default': 'Chatbot response will appear here.'}\n",
            "  outputs=[gr.Textbox(label=\"Chatbot Response\", type=\"text\", default=\"Chatbot response will appear here.\", lines=10)],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://dc79fd07c0a89f10de.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dc79fd07c0a89f10de.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://dc79fd07c0a89f10de.gradio.live\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class ChatbotHelper:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.chatbot_instance = None\n",
        "        self.chat_history = []\n",
        "        self.chunks = None\n",
        "\n",
        "    def find_urls(self, text: str) -> List[str]:\n",
        "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        return url_pattern.findall(text)\n",
        "\n",
        "    def initialize_chatbot(self, urls: List[str]):\n",
        "        documents = self.load_website_content(urls)\n",
        "        chunks = self.split_text(documents)\n",
        "        embedder = self.bge_embedding(chunks)\n",
        "        vectorstore = self.create_vector_store(chunks, embedder)\n",
        "        retriever = self.create_retriever(vectorstore)\n",
        "        self.chatbot_instance = self.create_chatbot(retriever)\n",
        "        return \"Chatbot initialized! How can I assist you? now ask your Quetions\"\n",
        "\n",
        "    def load_website_content(self, urls):\n",
        "        print(\"Loading website(s) into Documents...\")\n",
        "        documents = WebBaseLoader(web_path=urls).load()\n",
        "        print(\"Done loading website(s).\")\n",
        "        return documents\n",
        "\n",
        "    def load_llm(self):\n",
        "        llm = CTransformers(\n",
        "            model=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n",
        "            model_type=\"mistral\"\n",
        "        )\n",
        "        return llm\n",
        "\n",
        "    def split_text(self, documents):\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=120,\n",
        "            chunk_overlap=20,\n",
        "            length_function=len\n",
        "        )\n",
        "        chunks = text_splitter.transform_documents(documents)\n",
        "        print(\"Done splitting documents.\")\n",
        "        return chunks\n",
        "\n",
        "    def bge_embedding(self, chunks):\n",
        "        print(\"Creating bge embedder...\")\n",
        "        model_name = \"BAAI/bge-base-en\"\n",
        "        encode_kwargs = {'normalize_embeddings': True}\n",
        "        embedder = HuggingFaceBgeEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cpu'},\n",
        "            encode_kwargs=encode_kwargs\n",
        "        )\n",
        "        return embedder\n",
        "\n",
        "    def create_vector_store(self, chunks, embedder):\n",
        "        print(\"Creating vectorstore...\")\n",
        "        db = lancedb.connect('/tmp/lancedb')\n",
        "        table = db.create_table(\"pdf_search\", data=[\n",
        "            {\"vector\": embedder.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\"}\n",
        "        ], mode=\"overwrite\")\n",
        "        vectorstore = LanceDB.from_documents(chunks, embedder, connection=table)\n",
        "        return vectorstore\n",
        "\n",
        "    def create_retriever(self, vectorstore):\n",
        "        print(\"Creating vectorstore retriever...\")\n",
        "        retriever = vectorstore.as_retriever()\n",
        "        return retriever\n",
        "\n",
        "    def embed_user_query(self, query):\n",
        "        if self.chunks is None:\n",
        "            return \"Chatbot not initialized. Please provide a URL first.\"\n",
        "        core_embeddings_model = self.bge_embedding(self.chunks)\n",
        "        embedded_query = core_embeddings_model.embed_query(query)\n",
        "        return embedded_query\n",
        "\n",
        "    def create_chatbot(self, retriever):\n",
        "        llm = self.load_llm()\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history',\n",
        "            return_messages=True\n",
        "        )\n",
        "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            memory=memory\n",
        "        )\n",
        "        return conversation_chain\n",
        "\n",
        "    def chat(self, conversation_chain, input):\n",
        "        return conversation_chain.run(input)\n",
        "\n",
        "    def respond(self, message):\n",
        "        if message.lower() == \"clear\":\n",
        "            self.chatbot_instance = None\n",
        "            self.chat_history.clear()\n",
        "            return \"\", self.chat_history\n",
        "\n",
        "        urls = self.find_urls(message)\n",
        "\n",
        "        if not self.chatbot_instance and urls:\n",
        "            bot_message = self.initialize_chatbot(urls)\n",
        "        else:\n",
        "            if self.chatbot_instance:\n",
        "                bot_message = self.chat(self.chatbot_instance, message)\n",
        "            else:\n",
        "                bot_message = \"Please provide a URL to initialize the chatbot first, then ask any questions related to that site.\"\n",
        "\n",
        "        self.chat_history.append((message, bot_message))\n",
        "        chat_history_text = \"\\n\".join([f\"User: {msg[0]}\\nBot: {msg[1]}\\n\" for msg in self.chat_history])\n",
        "        return bot_message\n",
        "\n",
        "    def run_interface(self):\n",
        "\n",
        "\n",
        "        iface = gr.Interface(\n",
        "            fn=self.respond,\n",
        "            title=\"Chatbot with any url/website \",\n",
        "            inputs=gr.Textbox(label=\"Your Query\", placeholder=\"Type your query here...\",lines=5),\n",
        "            outputs=[gr.Textbox(label=\"Chatbot Response\", type=\"text\", default=\"Chatbot response will appear here.\", lines=10)],\n",
        "\n",
        "        )\n",
        "        iface.launch(debug=True,share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot_helper = ChatbotHelper()\n",
        "    chatbot_helper.run_interface()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-zM7psg2ZtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import openai\n",
        "import gradio as gr\n",
        "from typing import List, Union\n",
        "import lancedb\n",
        "import langchain\n",
        "from langchain.vectorstores import LanceDB\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "# # Set OpenAI API key as an environment variable\n",
        "# #os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "# #OPENAI_API_KEY  ='YOURTKEYpaste her-sk-9Tnu7YtoZNNqs\n",
        "\n",
        "\n",
        "\n",
        "def find_urls(text: str) -> List:\n",
        "\n",
        "    # Regular expression to match common URLs and ones starting with 'www.'\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.findall(text)\n",
        "\n",
        "# for pdf reading\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def website_loader(website: Union[str, list[str]]) -> List[langchain.schema.document.Document]:\n",
        "\n",
        "    print(\"Loading website(s) into Documents...\")\n",
        "    documents = WebBaseLoader(web_path=website).load()\n",
        "    print(\"Done loading website(s).\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "def split_text(documents: List) -> List[langchain.schema.document.Document]:\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=120,\n",
        "                                                   chunk_overlap=20,\n",
        "                                                   length_function=len\n",
        "                                                   )\n",
        "    chunks = text_splitter.transform_documents(documents)\n",
        "    print(\"Done splitting documents.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def bge_embedding(chunks:List):\n",
        "\n",
        "    print(\"Creating bge embedder...\")\n",
        "\n",
        "    model_name = \"BAAI/bge-base-en\"\n",
        "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "    embedder = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "\n",
        "    return embedder\n",
        "\n",
        "\n",
        "def create_vector_store(chunks: List[langchain.schema.document.Document],\n",
        "                        embedder):\n",
        "\n",
        "    print(\"Creating vectorstore...\")\n",
        "    #vectorstore = FAISS.from_documents(chunks, embedder)\n",
        "    #return vectorstore\n",
        "\n",
        "    db = lancedb.connect('/tmp/lancedb')\n",
        "    table = db.create_table(\"pdf_sear1ch\", data=[\n",
        "        {\"vector\": embedder.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\"}\n",
        "    ], mode=\"overwrite\")\n",
        "    vectorstore = LanceDB.from_documents(chunks, embedder, connection=table)\n",
        "    return vectorstore\n",
        "\n",
        "# download llm  model & put in working directory\n",
        "# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "\n",
        "def load_llm():\n",
        "    # Load the locally downloaded model here\n",
        "    llm = CTransformers(\n",
        "        model = \"mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n",
        "        model_type=\"mistral\"\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "\n",
        "def create_retriever(vectorstore) :\n",
        "\n",
        "    print(\"Creating vectorstore retriever...\")\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def embed_user_query(query: str) -> List[float]:\n",
        "\n",
        "    core_embeddings_model = bge_embedding(chunks)  # bge ranked top on leaderbord  https://huggingface.co/spaces/mteb/leaderboard\n",
        "    #HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    #                                model_kwargs={'device':\"cpu\"})\n",
        "\n",
        "    #OpenAIEmbeddings()\n",
        "    embedded_query = core_embeddings_model.embed_query(query)\n",
        "    return embedded_query\n",
        "\n",
        "def similarity_search(vectorstore: langchain.vectorstores,\n",
        "                      embedded_query: List[float]) -> List[langchain.schema.document.Document]:\n",
        "\n",
        "    response = vectorstore.similarity_search_by_vector(embedded_query, k=4)\n",
        "    return response\n",
        "\n",
        "\n",
        "def create_chatbot(retriever):\n",
        "\n",
        "    #llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "    llm = load_llm()\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history',\n",
        "        return_messages=True\n",
        "        )\n",
        "\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory\n",
        "        )\n",
        "    return conversation_chain\n",
        "\n",
        "def chat(conversation_chain, input: str) -> str:\n",
        "\n",
        "    return conversation_chain.run(input)\n",
        "\n",
        "text = \"\"\"\n",
        "your expert at answering the quetions which asked by user. I need you to go to the following URLs and get information about them\n",
        "https://blog.lancedb.com/llms-rag-the-missing-storage-layer-for-ai-28ded35fa984\n",
        "and this https://blog.lancedb.com/context-aware-chatbot-using-llama-2-lancedb-as-vector-database-4d771d95c755\n",
        "\n",
        "if you dont know the answer said that 'i dont know this dude'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# This chatbot_instance will be initialized once a URL is provided.\n",
        "chatbot_instance = None\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    global chatbot_instance\n",
        "    if message.lower() == \"clear\":\n",
        "        chatbot_instance = None  # Reset the chatbot instance\n",
        "        chat_history.clear()  # Clear chat history\n",
        "        return \"\", chat_history\n",
        "\n",
        "    urls = find_urls(message)\n",
        "\n",
        "    # If the chatbot is not yet initialized and we have URLs, initialize it\n",
        "    if not chatbot_instance and urls:\n",
        "        #documents = load_pdf(path)\n",
        "        documents = website_loader(urls)\n",
        "        chunks = split_text(documents)\n",
        "        embedder = bge_embedding(chunks)\n",
        "        vectorstore = create_vector_store(chunks, embedder)\n",
        "        retriever = create_retriever(vectorstore)\n",
        "        chatbot_instance = create_chatbot(retriever)\n",
        "        bot_message = \"Chatbot initialized ! Now how can I help you?\"\n",
        "    else:\n",
        "        if chatbot_instance:\n",
        "            bot_message = chat(chatbot_instance, message)\n",
        "        else:\n",
        "            bot_message = \"Please provide a URL to initialize the chatbot first, then ask any questions related to that site.\"\n",
        "\n",
        "    chat_history.append((message, bot_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(title=\"Chatbot with any url/website \",)\n",
        "    user_query = gr.Textbox(label=\"Your Query\", placeholder=\"How can I assist you today?\")\n",
        "    clear = gr.ClearButton([user_query, chatbot])\n",
        "\n",
        "    user_query.submit(respond, [user_query, chatbot], [user_query, chatbot])\n",
        "\n",
        "demo.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ytWlCnRE9DDW",
        "outputId": "f4bbe108-0167-4e1c-fe62-0566a3eded7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f27f4e4c29ffa11796.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f27f4e4c29ffa11796.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading website(s) into Documents...\n",
            "Done loading website(s).\n",
            "Done splitting documents.\n",
            "Creating bge embedder...\n",
            "Creating vectorstore...\n",
            "Creating vectorstore retriever...\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2365\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2366\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2b3f2510b0bb>\u001b[0m in \u001b[0;36m<cell line: 200>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrespond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, enable_queue, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, show_tips, height, width, encrypt, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, file_directories, allowed_paths, blocked_paths, root_path, _frontend, app_kwargs, state_session_capacity)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0;31m# Block main thread if debug==True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GRADIO_DEBUG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwasm_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIS_WASM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2260\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2261\u001b[0m         \u001b[0;31m# Block main thread if running in a script to stop script from exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0mis_in_interactive_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ps1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2368\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyboard interruption in main thread... closing server.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2371\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtunnel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCURRENT_TUNNELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0mtunnel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/networking.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cBzX94U9EW2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}