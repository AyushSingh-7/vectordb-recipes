{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cb272e",
   "metadata": {},
   "source": [
    "# Code documentation Q&A bot example with LangChain\n",
    "\n",
    "This Q&A bot will allow you to query your own documentation easily using questions. We'll also demonstrate the use of LangChain and LanceDB using the OpenAI API. \n",
    "\n",
    "In this example we'll use Pandas 2.0 documentation, but, this could be replaced for your own docs as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66638d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq openai langchain tiktoken\n",
    "!pip install -qq lancedb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cdcac3",
   "metadata": {},
   "source": [
    "First, let's get some setup out of the way. As we're using the OpenAI API, ensure that you've set your key (and organization if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ee1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Configuring the environment variable OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-..\"\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    \n",
    "assert len(openai.Model.list()[\"data\"]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1c54bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceTable(sup)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "db = lancedb.connect(\"data/sample-lancedb\")\n",
    "db.open_table(\"sup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f524d3",
   "metadata": {},
   "source": [
    "# Loading in our code documentation, generating embeddings and storing our documents in LanceDB\n",
    "\n",
    "We're going to use the power of LangChain to help us create our Q&A bot. It comes with several APIs that can make our development much easier as well as a LanceDB integration for vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55d22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc6d50",
   "metadata": {},
   "source": [
    "To make this easier, we've downloaded Pandas documentation and stored the raw HTML files for you to download. We'll download them and then use LangChain's HTML document readers to parse them and store them in LanceDB as a vector store, along with relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da77e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_docs = requests.get(\"https://eto-public.s3.us-west-2.amazonaws.com/datasets/pandas_docs/pandas.documentation.zip\")\n",
    "with open('/tmp/pandas.documentation.zip', 'wb') as f:\n",
    "    f.write(pandas_docs.content)\n",
    "\n",
    "file = zipfile.ZipFile(\"/tmp/pandas.documentation.zip\")\n",
    "file.extractall(path=\"/tmp/pandas_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42496c",
   "metadata": {},
   "source": [
    "We'll create a simple helper function that can help to extract metadata, so we can use this downstream when we're wanting to query with filters. In this case, we want to keep the lineage of the uri or path for each document that we process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d171d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_title(document):\n",
    "    m = str(document.metadata[\"source\"])\n",
    "    title = re.findall(\"pandas.documentation(.*).html\", m)\n",
    "    if title[0] is not None:\n",
    "        return(title[0])\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130162ad",
   "metadata": {},
   "source": [
    "# Pre-processing and loading the documentation\n",
    "\n",
    "Next, let's pre-process and load the documentation. To make sure we don't need to do this repeatedly if we were updating code, we're caching it using pickle so we can retrieve it again (this could take a few minutes to run the first time you do it). We'll also add some more metadata to the docs here such as the title and version of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33bfe7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = Path(\"docs.pkl\")\n",
    "docs = []\n",
    "\n",
    "if not docs_path.exists():\n",
    "    for p in Path(\"/tmp/pandas_docs/pandas.documentation\").rglob(\"*.html\"):\n",
    "        print(p)\n",
    "        if p.is_dir():\n",
    "            continue\n",
    "        try:\n",
    "            loader = UnstructuredHTMLLoader(p)\n",
    "            raw_document = loader.load()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading document: {e}\")\n",
    "            continue\n",
    "        \n",
    "        m = {}\n",
    "        m[\"title\"] = get_document_title(raw_document[0])\n",
    "        m[\"version\"] = \"2.0rc0\"\n",
    "        raw_document[0].metadata = raw_document[0].metadata | m\n",
    "        raw_document[0].metadata[\"source\"] = str(raw_document[0].metadata[\"source\"])\n",
    "        docs = docs + raw_document\n",
    "\n",
    "    with docs_path.open(\"wb\") as fh:\n",
    "        pickle.dump(docs, fh)\n",
    "else:\n",
    "    with docs_path.open(\"rb\") as fh:\n",
    "        docs = pickle.load(fh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3852dd3",
   "metadata": {},
   "source": [
    "# Generating emebeddings from our docs\n",
    "\n",
    "Now that we have our raw documents loaded, we need to pre-process them to generate embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82230563",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e68215",
   "metadata": {},
   "source": [
    "# Storing and querying with LanceDB\n",
    "\n",
    "Let's connect to LanceDB so we can store our documents. We'll create a Table to store them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74780a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect('lancedb')\n",
    "table = db.create_table(\"pandas_docs\", data=[\n",
    "    {\"vector\": embeddings.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\"}\n",
    "], mode=\"overwrite\")\n",
    "docsearch = LanceDB.from_documents(documents, embeddings, connection=table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1dc5d",
   "metadata": {},
   "source": [
    "Now let's create our RetrievalQA chain using the LanceDB vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5891ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d93b85",
   "metadata": {},
   "source": [
    "And thats it! We're all setup. The next step is to run some queries, let's try a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d88316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The major differences in pandas 2.0 include the ability to install optional dependencies with pip extras, the ability to use any numpy numeric dtype in an Index, and enhancements, notable bug fixes, backwards incompatible API changes, deprecations, removal of prior version deprecations/changes, and performance improvements.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the major differences in pandas 2.0?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a0397c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 2.0.0rc0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's the current version of pandas?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923f86c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You can install optional dependencies with pip install \"pandas[performance]\" and specific sets of dependencies are listed in the sections below. You can also import optional dependencies with pandas.compat._optional.import_optional_dependency and make sure to include a test asserting that an ImportError is raised when the optional dependency is not found. Additionally, make sure to document any optional dependencies in Optional Dependencies and set the minimum required version in the pandas.compat._optional.VERSIONS dict for backwards compatibility.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do I make use of installing optional dependencies?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02082f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Possible incompatibility for HDF5 formats created with pandas < 0.13.0, Map on Index types now return other Index types, Accessing datetime fields of Index now return Index, pd.unique will now be consistent with extension types, S3 file handling, Partial string indexing changes, Concat of different float dtypes will not automatically upcast, pandas Google BigQuery support has moved, Memory usage for Index is more accurate, DataFrame.sort_index changes, GroupBy describe formatting, Window binary corr/cov operations return a MultiIndex DataFrame, HDFStore where string comparison, Dependencies have increased minimum versions, Sum/prod of all-NaN or empty Series/DataFrames is now consistently NaN, Indexing with a list with missing labels is deprecated, NA naming changes, Iteration of Series/Index will now return Python scalars, Indexing with a Boolean Index, PeriodIndex resampling, Improved error handling during item assignment in pd.eval, Dtype conversions, MultiIndex constructor with a single level, UTC localization with Series, Consistency of range functions, No automatic Matplotlib converters, Instantiation from dicts preserves dict insertion order for Python 3.6+,'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the backwards incompatible API changes in Pandas 2.0?\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "511a7c77cb034b09af5465c01316a0f4bb20176d139e60e6d7915f9a637a5037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
