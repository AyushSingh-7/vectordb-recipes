{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d29cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/ayushchaurasia/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: lancedb in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (0.1.9)\n",
      "Requirement already satisfied: open_clip_torch in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (2.20.0)\n",
      "Collecting arxiv\n",
      "  Downloading arxiv-1.4.8-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pylance~=0.5.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from lancedb) (0.5.1)\n",
      "Requirement already satisfied: ratelimiter in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from lancedb) (1.2.0.post0)\n",
      "Requirement already satisfied: retry in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from lancedb) (0.9.2)\n",
      "Requirement already satisfied: tqdm in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from lancedb) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from lancedb) (3.8.4)\n",
      "Requirement already satisfied: pydantic in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from lancedb) (1.10.9)\n",
      "Requirement already satisfied: attr in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from lancedb) (0.3.2)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (0.15.2)\n",
      "Requirement already satisfied: regex in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (2023.6.3)\n",
      "Requirement already satisfied: ftfy in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (6.1.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (0.15.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (0.1.99)\n",
      "Requirement already satisfied: protobuf<4 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (3.20.3)\n",
      "Requirement already satisfied: timm in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from open_clip_torch) (0.9.5)\n",
      "Collecting feedparser (from arxiv)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=10 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from pylance~=0.5.0->lancedb) (12.0.1)\n",
      "Requirement already satisfied: pandas>=1.4 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from pylance~=0.5.0->lancedb) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from pylance~=0.5.0->lancedb) (1.23.5)\n",
      "Requirement already satisfied: filelock in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from aiohttp->lancedb) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from aiohttp->lancedb) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from aiohttp->lancedb) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from aiohttp->lancedb) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from aiohttp->lancedb) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from aiohttp->lancedb) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from aiohttp->lancedb) (1.3.1)\n",
      "Collecting sgmllib3k (from feedparser->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from ftfy->open_clip_torch) (0.2.6)\n",
      "Requirement already satisfied: fsspec in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (2023.6.0)\n",
      "Requirement already satisfied: requests in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (23.1)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from retry->lancedb) (5.1.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from retry->lancedb) (1.11.0)\n",
      "Requirement already satisfied: safetensors in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from timm->open_clip_torch) (0.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from torchvision->open_clip_torch) (9.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from pandas>=1.4->pylance~=0.5.0->lancedb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from pandas>=1.4->pylance~=0.5.0->lancedb) (2023.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->lancedb) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.5.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath>=0.19 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ayushchaurasia/Documents/vectordb-recipes/env/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas>=1.4->pylance~=0.5.0->lancedb) (1.16.0)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=294e6b709120203187d4be5f4d77a118173b88d4feee49642595b3060423a544\n",
      "  Stored in directory: /private/tmp/pip-ephem-wheel-cache-dyw0r0kc/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-1.4.8 feedparser-6.0.10 sgmllib3k-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "!pip install lancedb open_clip_torch arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94724aa",
   "metadata": {},
   "source": [
    "## Explain Open CLIP and embeddings here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78088422",
   "metadata": {},
   "source": [
    "## Creating table from arxiv API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cba25e",
   "metadata": {},
   "source": [
    "### Embedding Paper Summary using CLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fba615af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "from open_clip import tokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import arxiv\n",
    "import lancedb\n",
    "\n",
    "def embed_func_clip(text):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(tokenizer(text))\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5802574",
   "metadata": {},
   "source": [
    "### Create a DataFrame of the desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb8afe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_df(embed_func, length=10000):\n",
    "    results = arxiv.Search(\n",
    "      query= \"cat:cs.AI OR cat:cs.CV OR cat:stat.ML\",\n",
    "      max_results = length,\n",
    "      sort_by = arxiv.SortCriterion.Relevance,\n",
    "      sort_order = arxiv.SortOrder.Descending\n",
    "    ).results()\n",
    "    df = defaultdict(list)\n",
    "    for result in tqdm(results, total=length):\n",
    "        try:\n",
    "            df[\"title\"].append(result.title)\n",
    "            df[\"summary\"].append(result.summary)\n",
    "            df[\"authors\"].append(str(result.authors))\n",
    "            df[\"url\"].append(result.entry_id)\n",
    "            df[\"vector\"].append(embed_func(result.summary).tolist()[0])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error: \", e)\n",
    "    \n",
    "    return pd.DataFrame(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78ec90",
   "metadata": {},
   "source": [
    "### Create LanceDB Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa2edccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    db = lancedb.connect(\"db\")\n",
    "    df = get_arxiv_df(embed_func_clip)\n",
    "\n",
    "    tbl = db.create_table(\"arxiv\", data=df, mode=\"overwrite\")\n",
    "\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c04cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import build_table # helper local module\n",
    "\n",
    "db = lancedb.connect(\"db\")\n",
    "\n",
    "if \"arxiv\" not in db.table_names():\n",
    "    tbl = create_table()\n",
    "else:\n",
    "    tbl = db.open_table(\"arxiv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09adb9d3",
   "metadata": {},
   "source": [
    "## Semantic Search by concepts or summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acc38daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_table(query, embed_func=embed_func_clip, lim=3):\n",
    "    db = lancedb.connect(\"db\")\n",
    "    tbl = db.open_table(\"arxiv\")\n",
    "\n",
    "    embs = embed_func(query)\n",
    "    \n",
    "    return tbl.search(embs.tolist()[0]).limit(3).to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "971be6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>vector</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can SAM Count Anything? An Empirical Study on ...</td>\n",
       "      <td>Meta AI recently released the Segment Anything...</td>\n",
       "      <td>[arxiv.Result.Author('Zhiheng Ma'), arxiv.Resu...</td>\n",
       "      <td>http://arxiv.org/abs/2304.10817v1</td>\n",
       "      <td>[-0.023224907, 0.060689516, 0.14642626, -0.187...</td>\n",
       "      <td>35.039112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Communication-Computation Efficient Device-Edg...</td>\n",
       "      <td>Device-edge co-inference, which partitions a d...</td>\n",
       "      <td>[arxiv.Result.Author('Xinjie Zhang'), arxiv.Re...</td>\n",
       "      <td>http://arxiv.org/abs/2108.13009v2</td>\n",
       "      <td>[0.17511544, -0.14268509, 0.11111196, 0.025091...</td>\n",
       "      <td>35.435425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Saliency Prediction for Mobile User Interfaces</td>\n",
       "      <td>We introduce models for saliency prediction fo...</td>\n",
       "      <td>[arxiv.Result.Author('Prakhar Gupta'), arxiv.R...</td>\n",
       "      <td>http://arxiv.org/abs/1711.03726v3</td>\n",
       "      <td>[-0.23654917, 0.044955254, -0.060538974, -0.08...</td>\n",
       "      <td>36.065895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Can SAM Count Anything? An Empirical Study on ...   \n",
       "1  Communication-Computation Efficient Device-Edg...   \n",
       "2     Saliency Prediction for Mobile User Interfaces   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Meta AI recently released the Segment Anything...   \n",
       "1  Device-edge co-inference, which partitions a d...   \n",
       "2  We introduce models for saliency prediction fo...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [arxiv.Result.Author('Zhiheng Ma'), arxiv.Resu...   \n",
       "1  [arxiv.Result.Author('Xinjie Zhang'), arxiv.Re...   \n",
       "2  [arxiv.Result.Author('Prakhar Gupta'), arxiv.R...   \n",
       "\n",
       "                                 url  \\\n",
       "0  http://arxiv.org/abs/2304.10817v1   \n",
       "1  http://arxiv.org/abs/2108.13009v2   \n",
       "2  http://arxiv.org/abs/1711.03726v3   \n",
       "\n",
       "                                              vector      score  \n",
       "0  [-0.023224907, 0.060689516, 0.14642626, -0.187...  35.039112  \n",
       "1  [0.17511544, -0.14268509, 0.11111196, 0.025091...  35.435425  \n",
       "2  [-0.23654917, 0.044955254, -0.060538974, -0.08...  36.065895  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MobileSAM paper abstract 2nd half\n",
    "query = \"\"\"\n",
    "Many of such applications need to be run on resource-constraint edge devices,\n",
    "like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight\n",
    "image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM\n",
    "paper leads to unsatisfactory performance, especially when limited training sources are available. We\n",
    "find that this is mainly caused by the coupled optimization of the image encoder and mask decoder,\n",
    "motivated by which we propose decoupled distillation. Concretely, we distill the knowledge from\n",
    "the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be\n",
    "automatically compatible with the mask decoder in the original SAM. The training can be completed\n",
    "on a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM\n",
    "which is more than 60 times smaller yet performs on par with the original SAM. For inference speed,\n",
    "With a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms\n",
    "on the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the\n",
    "concurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover,\n",
    "we show that MobileSAM can run relatively smoothly on CPU\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "search_table(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf405ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>vector</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Attack-SAM: Towards Attacking Segment Anything...</td>\n",
       "      <td>Segment Anything Model (SAM) has attracted sig...</td>\n",
       "      <td>[arxiv.Result.Author('Chenshuang Zhang'), arxi...</td>\n",
       "      <td>http://arxiv.org/abs/2305.00866v2</td>\n",
       "      <td>[0.01118099, -0.0053622816, 0.0686877, -0.2128...</td>\n",
       "      <td>22.285025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast Segment Anything</td>\n",
       "      <td>The recently proposed segment anything model (...</td>\n",
       "      <td>[arxiv.Result.Author('Xu Zhao'), arxiv.Result....</td>\n",
       "      <td>http://arxiv.org/abs/2306.12156v1</td>\n",
       "      <td>[0.036730755, -0.067693904, -0.11393009, -0.11...</td>\n",
       "      <td>22.613714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAM on Medical Images: A Comprehensive Study o...</td>\n",
       "      <td>The Segment Anything Model (SAM) made an eye-c...</td>\n",
       "      <td>[arxiv.Result.Author('Dongjie Cheng'), arxiv.R...</td>\n",
       "      <td>http://arxiv.org/abs/2305.00035v1</td>\n",
       "      <td>[-0.2498389, 0.11826321, -0.117645726, -0.1427...</td>\n",
       "      <td>23.955942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Attack-SAM: Towards Attacking Segment Anything...   \n",
       "1                              Fast Segment Anything   \n",
       "2  SAM on Medical Images: A Comprehensive Study o...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Segment Anything Model (SAM) has attracted sig...   \n",
       "1  The recently proposed segment anything model (...   \n",
       "2  The Segment Anything Model (SAM) made an eye-c...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [arxiv.Result.Author('Chenshuang Zhang'), arxi...   \n",
       "1  [arxiv.Result.Author('Xu Zhao'), arxiv.Result....   \n",
       "2  [arxiv.Result.Author('Dongjie Cheng'), arxiv.R...   \n",
       "\n",
       "                                 url  \\\n",
       "0  http://arxiv.org/abs/2305.00866v2   \n",
       "1  http://arxiv.org/abs/2306.12156v1   \n",
       "2  http://arxiv.org/abs/2305.00035v1   \n",
       "\n",
       "                                              vector      score  \n",
       "0  [0.01118099, -0.0053622816, 0.0686877, -0.2128...  22.285025  \n",
       "1  [0.036730755, -0.067693904, -0.11393009, -0.11...  22.613714  \n",
       "2  [-0.2498389, 0.11826321, -0.117645726, -0.1427...  23.955942  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"Segment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot\n",
    "transfer performance and high versatility for numerous vision applications (like image editing with\n",
    "fine-grained control). Many of such applications need to be run on resource-constraint edge devices,\n",
    "like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight\n",
    "image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM\n",
    "paper leads to unsatisfactory performance, especially when limited training sources are available. We\n",
    "find that this is mainly caused by the coupled optimization of the image encoder and mask decoder,\n",
    "motivated by which we propose decoupled distillation\"\"\"\n",
    "\n",
    "search_table(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4ccd273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>vector</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Towards the Generalization of Contrastive Self...</td>\n",
       "      <td>Recently, self-supervised learning has attract...</td>\n",
       "      <td>[arxiv.Result.Author('Weiran Huang'), arxiv.Re...</td>\n",
       "      <td>http://arxiv.org/abs/2111.00743v4</td>\n",
       "      <td>[0.33684078, 0.12271507, -0.2530719, 0.3336045...</td>\n",
       "      <td>29.067186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Many-Goals Reinforcement Learning</td>\n",
       "      <td>All-goals updating exploits the off-policy nat...</td>\n",
       "      <td>[arxiv.Result.Author('Vivek Veeriah'), arxiv.R...</td>\n",
       "      <td>http://arxiv.org/abs/1806.09605v1</td>\n",
       "      <td>[0.081662394, -0.0060318857, -0.20181663, -0.1...</td>\n",
       "      <td>30.358921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efficient Self-supervised Continual Learning w...</td>\n",
       "      <td>Inspired by the success of Self-supervised lea...</td>\n",
       "      <td>[arxiv.Result.Author('Li Yang'), arxiv.Result....</td>\n",
       "      <td>http://arxiv.org/abs/2303.07477v1</td>\n",
       "      <td>[0.16743122, 0.21459761, 0.09188169, 0.3162266...</td>\n",
       "      <td>32.915543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Towards the Generalization of Contrastive Self...   \n",
       "1                  Many-Goals Reinforcement Learning   \n",
       "2  Efficient Self-supervised Continual Learning w...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Recently, self-supervised learning has attract...   \n",
       "1  All-goals updating exploits the off-policy nat...   \n",
       "2  Inspired by the success of Self-supervised lea...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [arxiv.Result.Author('Weiran Huang'), arxiv.Re...   \n",
       "1  [arxiv.Result.Author('Vivek Veeriah'), arxiv.R...   \n",
       "2  [arxiv.Result.Author('Li Yang'), arxiv.Result....   \n",
       "\n",
       "                                 url  \\\n",
       "0  http://arxiv.org/abs/2111.00743v4   \n",
       "1  http://arxiv.org/abs/1806.09605v1   \n",
       "2  http://arxiv.org/abs/2303.07477v1   \n",
       "\n",
       "                                              vector      score  \n",
       "0  [0.33684078, 0.12271507, -0.2530719, 0.3336045...  29.067186  \n",
       "1  [0.081662394, -0.0060318857, -0.20181663, -0.1...  30.358921  \n",
       "2  [0.16743122, 0.21459761, 0.09188169, 0.3162266...  32.915543  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search via a concept you're reading\n",
    "query = \"\"\"\n",
    "What is the general idea behin self-supervised learning.\n",
    "\"\"\"\n",
    "\n",
    "search_table(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f471b55",
   "metadata": {},
   "source": [
    "### Full Text Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
