This series will take you through all the building blocks of deep learning you need to know to implement CLIP or other multi-modal models from scratch and train them successfully avoiding the common pitfalls.

This takes a code first approach - you'll build eacah building block annotated with dimensions at every step along with intuition for each step.

Pre-requisites:
Not much but some familiarity with pytorch and DL knowledge till basics of CNNs and RRNs will help.

The concept breakdown will include:
1 - Attention Mechanism: Learn about attention, multi-head attention with masking with implementation and intuition

2 - Transformers from scratch - WIP

3 - Contrastive learning - WIP

4 - Reproducing CLIP/ loading official weights

5 - Training on mini-LAION dataset



## All-in-one blog - [Coming Soon]
All the topics covered as a blog in a single walkthrough.